import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
(X_train , Y_train) , (X_test , Y_test) = keras.datasets.mnist.load_data() #mnist is the dataset of handwritten digit classification from image
len(X_train)
len(Y_train)
len(X_test)
len(Y_test)
X_train[0].shape
X_train[0]
plt.matshow(X_train[0])
Y_train[0]
Y_train[:]
X_train = X_train/255
X_test = X_test/255
X_train_flatten = X_train.reshape(len(X_train) , 28*28)
print(X_train_flatten)
X_test_flatten = X_test.reshape(len(X_test) , 28*28) #flatten means 2d array to 1 d array
print(X_test_flatten)
model = keras.Sequential(
    [
        keras.Input(shape=(784,)),
        keras.layers.Dense(10, activation='sigmoid') # here 10 is output shape , dense means 1 layer's neurons are connected wid the 2nd layer's neurons
    ]
)
model.compile(optimizer='adam' , loss='sparse_categorical_crossentropy' , metrics=['accuracy']) #optimizer allows you to go to he lobal optima in efficient way ,
                                                                                    #loss error like mean absolute error , output class is categorical here , sparse means Y_train is int
                                                                                    # as my goal is to make it more accurate so we are using metrics

model.fit(X_train_flatten , Y_train , epochs=5) # here training actually happens , epochs is for no of itreation

model.evaluate(X_test_flatten , Y_test) # here testing actually happens on the test dataset

plt.matshow(X_test_flatten[0].reshape(28,28))
Y_pred =model.predict(X_test_flatten[0].reshape(1,784)) # it is printing ten scores
print(Y_pred)
np.argmax(Y_pred)
Y_pred = model.predict(X_test_flatten)
Y_pred1 = [np.argmax(i) for i in Y_pred]
tf.math.confusion_matrix(labels = Y_test , predictions = Y_pred1)
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(tf.math.confusion_matrix(labels = Y_test , predictions = Y_pred1) , annot=True , fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')   #it is telling that 964 t5imes model was 0 etc ,   anything the number is not there colourful then those were wrong
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential(
    [
        keras.Input(shape=(784,)),
        keras.layers.Dense(100, activation='relu'), # here 10 is output shape , dense means 1 layer's neurons are connected wid the 2nd layer's neurons , relu is the activation function
        keras.layers.Dense(10, activation='sigmoid')
    ]
)

model.compile(optimizer='adam' , loss='sparse_categorical_crossentropy' , metrics=['accuracy']) #optimizer allows you to go to he lobal optima in efficient way ,
                                                                                    #loss error like mean absolute error , output class is categorical here , sparse means Y_train is int
                                                                                    # as my goal is to make it more accurate so we are using metrics

model.fit(X_train_flatten , Y_train , epochs=5) # here training actually happens , epochs is for no of itreation

import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(tf.math.confusion_matrix(labels = Y_test , predictions = Y_pred1) , annot=True , fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')   #it is telling that 964 t5imes model was 0 etc ,   anything the number is not there colourful then those were wrong

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential(
    [
        keras.layers.Flatten(input_shape=(28,28)),
        keras.layers.Dense(100, activation='relu'), # here 10 is output shape , dense means 1 layer's neurons are connected wid the 2nd layer's neurons , relu is the activation function
        keras.layers.Dense(10, activation='sigmoid')
    ]
)

model.compile(optimizer='adam' , loss='sparse_categorical_crossentropy' , metrics=['accuracy']) #optimizer allows you to go to he lobal optima in efficient way ,
                                                                                    #loss error like mean absolute error , output class is categorical here , sparse means Y_train is int
                                                                                    # as my goal is to make it more accurate so we are using metrics

model.fit(X_train , Y_train , epochs=5)
import math
def sigmoid(x):
   return 1/(1+math.exp(-x))
sigmoid(100)
def tanh(x):
    return (math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))
tanh(100)
def relu(x):
    return max(0,x)

relu(100)
def leaky_relu(x):
    return max(0.1*x , x)
leaky_relu(-100)
coef , intercept = model.get_weights()
coef , intercept
!pip install matplotlib

import matplotlib.pyplot as plt

def prediction_function(image):
    prediction = model.predict(image)
    print(f"Prediction: {prediction}")

    print(f"Image shape: {image[0].shape}")  # Check the shape of the image data
    print(f"Image data type: {image[0].dtype}")  # Check the data type of the image

    plt.matshow(image[0])
    plt.show()
import matplotlib
matplotlib.use('Agg')  # Try another backend like 'Qt5Agg' if this doesn't work
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss
def sigmoid_numpy(X):
 return 1/(1+np.exp(-X))

sigmoid_numpy(np.array([12,0,1]))
def sigmoid_numpy(X):
   return 1/(1+np.exp(-X))

sigmoid_numpy(np.array([12,0,1]))

def log_loss(y_true, y_predicted):
    epsilon = 1e-15
    y_predicted_new = [max(i,epsilon) for i in y_predicted]
    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    y_predicted_new = np.array(y_predicted_new)
    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))


class myNN:
    def __init__(self):
        self.w1 = 1
        self.w2 = 1
        self.bias = 0

    def fit(self, X, y, epochs, loss_thresold):
        self.w1, self.w2, self.bias = self.gradient_descent(X['age'],X['affordibility'],y, epochs, loss_thresold)
        print(f"Final weights and bias: w1: {self.w1}, w2: {self.w2}, bias: {self.bias}")

    def predict(self, X_test):
        weighted_sum = self.w1*X_test['age'] + self.w2*X_test['affordibility'] + self.bias
        return sigmoid_numpy(weighted_sum)

    def gradient_descent(self, age,affordability, y_true, epochs, loss_thresold):
        w1 = w2 = 1
        bias = 0
        rate = 0.5
        n = len(age)
        for i in range(epochs):
            weighted_sum = w1 * age + w2 * affordability + bias
            y_predicted = sigmoid_numpy(weighted_sum)
            loss = log_loss(y_true, y_predicted)

            w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))
            w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))

            bias_d = np.mean(y_predicted-y_true)
            w1 = w1 - rate * w1d
            w2 = w2 - rate * w2d
            bias = bias - rate * bias_d

            if i%50==0:
                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')

            if loss<=loss_thresold:
                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')
                break

        return w1, w2, bias

    customModel = myNN()
customModel.fit(X_train_scaled, y_train, epochs=8000, loss_thresold=0.4631)

customModel.predict(X_test_scaled)

